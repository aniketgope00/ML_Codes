{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom ucimlrepo import fetch_ucirepo\\n#fetch dataset\\nbanknote_authentication = fetch_ucirepo(id = 267)\\nX = banknote_authentication.data.features\\ny = banknote_authentication.data.targets\\nprint(\"Features: \", list(X.columns))\\nprint(\"Target: \", list(y.columns))\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "#fetch dataset\n",
    "banknote_authentication = fetch_ucirepo(id = 267)\n",
    "X = banknote_authentication.data.features\n",
    "y = banknote_authentication.data.targets\n",
    "print(\"Features: \", list(X.columns))\n",
    "print(\"Target: \", list(y.columns))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for array operations\n",
    "import pandas as pd # create and use dataframes\n",
    "import matplotlib.pyplot as plt # for graphical plots\n",
    "import matplotlib.colors as mcolors # map the color of scatter dots in biplot\n",
    "import matplotlib.patches as patches # outline the vectors in a different color in the biplot\n",
    "import joblib # saving and loading py objects (especially large data structures)\n",
    "from scipy.stats import bartlett # for performing multicolinearity tests\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo # for performing multicolinearity tests\n",
    "from sklearn.decomposition import PCA # for performing Principal Component Analysis\n",
    "from sklearn.preprocessing import StandardScaler # to standardize variables\n",
    "from sklearn.model_selection import train_test_split # to split training and test sets\n",
    "from sklearn.ensemble import RandomForestClassifier # classifier algorithm\n",
    "from sklearn.metrics import accuracy_score, classification_report # evaluate results\n",
    "from sklearn.utils import resample # for balancing data (going to resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp = pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['variance', 'skewness', 'curtosis', 'entropy']\n",
    "target = 'class'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_info(df, name):\n",
    "    print(f'{name} Dataset Info:', end = '\\n')\n",
    "    print(df.info(), end = '\\n')\n",
    "    print(f'{name} Dataset First 5 Columns', end = '\\n')\n",
    "    print(df.head(), end = '\\n')\n",
    "    print('Missing Values Per Column:', end = '\\n')\n",
    "    print(df.isna().sum())\n",
    "    print(f'{name} Dataset Summary Description Statisitics:', end = '\\n')\n",
    "    print(df.describe(include = all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df):\n",
    "    #rename columns\n",
    "    df.rename(columns = {'curtosis':'kurtosis'}, inplace = True)\n",
    "    #shuffle dataframe\n",
    "    df_shuffle = df.sample(frac = 1, random_state = 37).reset_index(drop = True)\n",
    "    return df_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df_shuffle, validation_fraction = 0.2):\n",
    "    split_index = int(validation_fraction * len(df_shuffle))\n",
    "    df_val = df_shuffle.iloc[:split_index].reset_index(drop = True)\n",
    "    df_train_test = df_shuffle.iloc[split_index:].reset_index(drop = True)\n",
    "    return df_val, df_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_data():\n",
    "    # retrieve dataframe\n",
    "    df = df.get_dataframe()\n",
    "    #preprocess dataframe\n",
    "    df_shuffled = preprocess_dataframe(df)\n",
    "    df_val, _ = split_dataframe(df_shuffled)\n",
    "    X_val = df_val[features]\n",
    "    y_val = df_val[target]\n",
    "\n",
    "    return df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, title):\n",
    "    #histograms\n",
    "    columns_to_plot = [col for col in df.columns if col != 'class']\n",
    "    #adjust figure size to fit number of plots\n",
    "    columns_to_plot = columns_to_plot[:4]\n",
    "    num_columns = min(len(columns_to_plot), 2)\n",
    "    num_rows = (len(columns_to_plot) + num_columns - 1) // num_columns\n",
    "    #adjustment\n",
    "    plt.figure(figsize=(num_columns*4, num_rows*4)) #create subplot for each column\n",
    "    for i, column in enumerate(columns_to_plot):\n",
    "        plt.subplot(num_rows, num_columns, i+1) # create a subplot for each column\n",
    "        data = df[column] # extract data from column\n",
    "        n = len(data) # number of data points\n",
    "        n_bins = int(np.sqrt(n)) # number of bins to use\n",
    "        if n_bins < 1: # atleast 1 bin is used\n",
    "            n_bins = 1\n",
    "        data.hist(bins = n_bins, edgecolor = 'black')\n",
    "        plt.title(column)\n",
    "        plt.xlabel('value')\n",
    "        plt.ylabel('frequency')\n",
    "    plt.suptitle(f'Histograms of the {title} Set', fontsize = 20)\n",
    "    plt.tight_layout( rect=[0, 0, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multicolinearity_tests(df):\n",
    "    df_selected = df[features]\n",
    "    df_selected = df_selected.apply(pd.to_numeric, errors = 'coerce').dropna()\n",
    "    if df_selected.shape[0] < 2:\n",
    "        print(\"Not Enough Data for KMO Tests after dropping NaNs\")\n",
    "        return None\n",
    "    #Bartlett's Sphericity Test\n",
    "    stat, p_value = bartlett(*[df_selected[col] for col in features])\n",
    "    #printing results\n",
    "    print(\"Bartlett's Sphericity Test:\", end = '\\n')\n",
    "    print(\"Batlett's Test Statistics: \", stat, end = '\\n')\n",
    "    print(\"Bartlett's p-value: \", p_value)\n",
    "    #set significance level\n",
    "    alpha = 0.05\n",
    "    print(\"Significance Level: \", alpha, end = '\\n')\n",
    "\n",
    "    if p_value < alpha:\n",
    "        print(f'p=value ({p_value:.df}) < alpha ({alpha})', end = '\\n')\n",
    "        print(\"Favour H1 - at least 2 variances are different.\\n\")\n",
    "        print(\"Decision: Reject the null hypothesis of equal variances.\\n\")\n",
    "        print(\"Variances are heterogeneous.\\n\")\n",
    "        print(\"Implication: The correlation matrix is significantly different from an identity matrix.\\n\")\n",
    "        print(\"This suggests that there are meaningful correlation among the variables.\\n\")\n",
    "        print(\"You may proceed with PCA.\\n\")\n",
    "    else:\n",
    "        print(f'p-value ({p_value}) >= alpha ({alpha}) \\n')\n",
    "        print(\"Favour H0 - all variances are the same.\\n\")\n",
    "        print(\"Decision: Fail to reject the null hypothesis of equal variances.\")\n",
    "        print(\"Variances are homogeneous.\")\n",
    "        print(\"Implication: There is insufficient eveidence to say the correlation matrix is different from the identity matrix.\\n\")\n",
    "        print(\"This suggests that the variables may not be significantly correlated.\\n\")\n",
    "        print(\"PCA may not be appropriate, consider other methods or further investigation.\\n\")\n",
    "    #KMO\n",
    "    print(\"KMO Test:\\n\")\n",
    "    kmo_all, kmo_model = calculate_kmo(df_selected)\n",
    "\n",
    "    #create a dataframe to display KMO statistics\n",
    "    kmo_df = pd.DataFrame({\n",
    "        'Variable' : features,\n",
    "        'KMO Statistics': kmo_all\n",
    "    })\n",
    "\n",
    "    #print KMO decision rule\n",
    "    print(\"KMO Decision Rule:\\n\")\n",
    "    print(\"0.8 - 1.0: Excellent\\n\")\n",
    "    print(\"0.7 - 0.79: Good\\n\")\n",
    "    print(\"0.6 - 0.69: Mediocre\\n\")\n",
    "    print(\"0.5 - 0.59: Poor\\n\")\n",
    "    print(\"Below 0.5: Unacceptable\\n\")\n",
    "\n",
    "    for variable, kmo_value in zip(kmo_df['Variable'], kmo_df['KMO Statisitics']):\n",
    "        print(f\"{variable} : {kmo_value}\\n\")\n",
    "\n",
    "    #print overall KMO statistics\n",
    "    print(\"KMO statistics(overall)\", kmo_model)\n",
    "    if kmo_model >= 0.8:\n",
    "        conclusion = \"Excellent - Data is suitable for factor anaalysis.\\n\"\n",
    "    elif kmo_model >= 0.7:\n",
    "        conclusion = \"Good - Data is likely suitable for factor analysis.\\n\"\n",
    "    elif kmo_model >= 0.6:\n",
    "        conclusion = \"Mediocre - Data may be suitable for factor analysis.\\n\"\n",
    "    elif kmo_model >= 0.5:\n",
    "        conclusion = \"Poor - Data is not very suitable for factor analysis.\\n\"\n",
    "    else:\n",
    "        conclusion = \"Unacceptable - Data is not suitable for factor analysis.\\n\"\n",
    "    \n",
    "    #print conclusion\n",
    "    print(\"KMO Conclusion: \", conclusion, end = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X_train, X_test = None):\n",
    "    '''\n",
    "    X_train: training feature matrix to be standardized.\n",
    "    X_test: test feature matrix to be standardized.\n",
    "\n",
    "    Returns:\n",
    "    X_train_scaled: The standardized training feature matrix.\n",
    "    X_test_scaled: The standardized test feature matrix (if X_test is provided).\n",
    "    scaler: The StandardScaler object used for scaling.\n",
    "    \n",
    "    '''\n",
    "    #Initialize the standardscaler to standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    #Fit the scaler to the training data and transform it\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    #Transform the test data using the same scaler\n",
    "    X_test_scaled = scaler.transform(X_test) if X_test is not None else None\n",
    "    return X_train_scaled, X_test_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_analysis(X_train, X_test = None, X = None, y_train = None, n_components = 0.90):\n",
    "    '''\n",
    "    X_train : the training features matrix.\n",
    "    X_test : the test feature matrix.\n",
    "    n_components = the threshold, we establish this at 90%.\n",
    "\n",
    "    Returns:\n",
    "    X_train_pca: the training data transformed into principal components.\n",
    "    X_test_pca: the testing data transformed into principal components.\n",
    "    pca: the PCA object used for dimensionality reduction.\n",
    "    '''\n",
    "    X_train = X_train.apply(pd.to_numeric, errors = 'coerce')\n",
    "    if X_test not in None:\n",
    "        X_test = X_test.apply(pd.to_numeric, errors = 'coerce')\n",
    "    #standardize data\n",
    "    X_train_scaled, X_test_scaled, scaler = standardize_data(X_train, X_test)\n",
    "    #perform PCA\n",
    "    pca_intial = PCA()\n",
    "    X_train_pca_initial = pca_intial.fit_transform(X_train_scaled)\n",
    "    explained_variance_ratio = pca_intial.explained_variance_ratio_\n",
    "    #calculate the cumulative explained variance to determine the number of components\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "    num_components = np.argmax(cumulative_explained_variance >= n_components) + 1\n",
    "    #perform PCA with determined number of components.\n",
    "    pca = PCA(n_components = num_components)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_scaled = pca.transform(X_test_scaled) if X_test_scaled is not None else None\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
